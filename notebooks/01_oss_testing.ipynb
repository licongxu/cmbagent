{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## CMBAgent - OSS Testing\n",
    "\n",
    "*For testing purposes only, to be removed before opening a PR to* `main` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local model testing supported via https://github.com/acceleratescience/mini-vllm (private Accelerate repo)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BASE_URL = os.getenv(\"LOCAL_LLM_BASE_URL\")\n",
    "API_KEY = os.getenv(\"LOCAL_LLM_API_KEY\")\n",
    "\n",
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"openai/gpt-oss-20b\", # Must match model id from endpoint\n",
    "            \"api_key\": API_KEY,\n",
    "            \"api_type\": \"openai\",\n",
    "            \"base_url\": BASE_URL,\n",
    "            \"price\": [0.0, 0.0], # Passed explicitly to ignore warning\n",
    "        },\n",
    "    ],\n",
    "    \"cache_seed\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### AG2 ConversableAgent() with `openai/gpt-oss-20b`\n",
    "\n",
    "Simple chat with a tiny model (`openai/gpt-oss-20b`)to verify ConversableAgent() has OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "What is the meaning of Denario?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b agent 0.00000             86                219           305\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "**Denario** (from Latin *denarius*) is a silver coin that served as the basic unit of currency in ancient Rome. It was a common everyday piece of money used for trade and paying wages. In later periods the word also came to refer broadly to a small amount of money.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (25601e1c-8e68-4851-a444-81ca04a96109): Maximum turns (2) reached\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=7595919412255891713458123805495119465, chat_history=[{'content': 'What is the meaning of Denario?', 'role': 'assistant', 'name': 'agent'}, {'content': '', 'role': 'user', 'name': 'user'}, {'content': '**Denario** (from Latin *denarius*) is a silver coin that served as the basic unit of currency in ancient Rome. It was a common everyday piece of money used for trade and paying wages. In later periods the word also came to refer broadly to a small amount of money.', 'role': 'assistant', 'name': 'agent'}, {'content': '', 'role': 'user', 'name': 'user'}], summary='', cost={'usage_including_cached_inference': {'total_cost': 0.0, 'openai/gpt-oss-20b': {'cost': 0.0, 'prompt_tokens': 86, 'completion_tokens': 219, 'total_tokens': 305}}, 'usage_excluding_cached_inference': {'total_cost': 0.0, 'openai/gpt-oss-20b': {'cost': 0.0, 'prompt_tokens': 86, 'completion_tokens': 219, 'total_tokens': 305}}}, human_input=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen import UserProxyAgent, ConversableAgent\n",
    "\n",
    "# Creates the agent that uses the vLLM LM\n",
    "assistant = ConversableAgent(\n",
    "    \"agent\",\n",
    "    llm_config=local_llm_config,\n",
    "    system_message=\"Answer briefly.\",\n",
    ")\n",
    "\n",
    "# Create the agent that represents the user in the conversation\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# User asks a question\n",
    "assistant.initiate_chat(\n",
    "    user_proxy,\n",
    "    message=\"What is the meaning of Denario?\",\n",
    "    max_turns=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### AG2 ConversableAgent() Tool Calling with OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "Use the add tool to add 2 and 3. Then reply with just the number.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b agent 0.00000            175                 35           210\n",
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (chatcmpl-tool-a10236a53e17d3d0): add *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 2, \"b\": 3}\n",
      "\u001b[32m*********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION add...\n",
      "Call ID: chatcmpl-tool-a10236a53e17d3d0\n",
      "Input arguments: {'a': 2, 'b': 3}\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTED FUNCTION add...\n",
      "Call ID: chatcmpl-tool-a10236a53e17d3d0\n",
      "Input arguments: {'a': 2, 'b': 3}\n",
      "Output:\n",
      "5.0\u001b[0m\n",
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "\u001b[32m***** Response from calling Tool tool (chatcmpl-tool-a10236a53e17d3d0) *****\u001b[0m\n",
      "5.0\n",
      "\u001b[32m****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b agent 0.00000            426                 40           466\n",
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b agent 0.00000            689                167           856\n",
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "Got it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b agent 0.00000            968                215          1183\n",
      "\u001b[33magent\u001b[0m (to agent):\n",
      "\n",
      "Great! If you have any more questions, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (dfb67d60-1141-48cf-838c-a7667c8d3808): Maximum turns (3) reached\u001b[0m\n",
      "[{'id': 'chatcmpl-tool-a10236a53e17d3d0', 'function': {'arguments': '{\"a\": 2, \"b\": 3}', 'name': 'add'}, 'type': 'function'}]\n",
      "[{'id': 'chatcmpl-tool-a10236a53e17d3d0', 'function': {'arguments': '{\"a\": 2, \"b\": 3}', 'name': 'add'}, 'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "assistant.register_for_llm(name=\"add\", description=\"Add two numbers\")(add)\n",
    "assistant.register_for_execution(name=\"add\")(add)\n",
    "\n",
    "result = assistant.initiate_chat(\n",
    "    recipient=assistant,\n",
    "    message=\"Use the add tool to add 2 and 3. Then reply with just the number.\",\n",
    "    max_turns=3,\n",
    ")\n",
    "\n",
    "for msg in result.chat_history:\n",
    "    if msg.get(\"tool_calls\"):\n",
    "        print(msg[\"tool_calls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### OSS with Engineer Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m_User\u001b[0m (to main_cmbagent_chat):\n",
      "\n",
      "\n",
      "Plot the graph of y = x from x = 0 to 1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: engineer\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "             Model    agent    Cost  Prompt Tokens  Completion Tokens  Total Tokens\n",
      "openai/gpt-oss-20b engineer 0.00000           1768               1488          3256\n",
      "\u001b[33mengineer\u001b[0m (to main_cmbagent_chat):\n",
      "\n",
      "**Code Explanation:**\n",
      "\n",
      "The script generates a simple linear plot of y = x over the interval [0, 1]. It creates a `data` directory if it does not exist, constructs the plot with Matplotlib, and saves it as a high‑resolution PNG file (300 dpi) in that directory. The filename includes a timestamp to ensure uniqueness. After saving, the script prints a concise description of the plot and the file path.\n",
      "\n",
      "**Modifications:**  \n",
      "None.\n",
      "\n",
      "**Python Code:**\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Plot y = x from 0 to 1 and save to data folder.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import datetime\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams\n",
      "\n",
      "rcParams['text.usetex'] = False\n",
      "\n",
      "def main():\n",
      "    data_dir = 'data'\n",
      "    if not os.path.isdir(data_dir):\n",
      "        os.makedirs(data_dir)\n",
      "    x = np.linspace(0, 1, 100)\n",
      "    y = x\n",
      "    plt.figure(figsize=(6, 4))\n",
      "    plt.plot(x, y, label='y = x')\n",
      "    plt.xlabel('x')\n",
      "    plt.ylabel('y')\n",
      "    plt.title('Plot of y = x')\n",
      "    plt.grid(True)\n",
      "    plt.legend()\n",
      "    plt.tight_layout()\n",
      "    print('Plot of y = x from x=0 to 1')\n",
      "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
      "    filename = 'plot_y_equals_x_1_' + timestamp + '.png'\n",
      "    filepath = os.path.join(data_dir, filename)\n",
      "    plt.savefig(filepath, dpi=300)\n",
      "    plt.close()\n",
      "    print('Plot saved to ' + filepath)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m***** AfterWork handoff (engineer): engineer_nest *****\u001b[0m\n",
      "\u001b[32m\n",
      "Next speaker: engineer_nest\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mengineer_nest\u001b[0m (to engineer_nested_chat):\n",
      "\n",
      "**Code Explanation:**\n",
      "\n",
      "The script generates a simple linear plot of y = x over the interval [0, 1]. It creates a `data` directory if it does not exist, constructs the plot with Matplotlib, and saves it as a high‑resolution PNG file (300 dpi) in that directory. The filename includes a timestamp to ensure uniqueness. After saving, the script prints a concise description of the plot and the file path.\n",
      "\n",
      "**Modifications:**  \n",
      "None.\n",
      "\n",
      "**Python Code:**\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Plot y = x from 0 to 1 and save to data folder.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import datetime\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import rcParams\n",
      "\n",
      "rcParams['text.usetex'] = False\n",
      "\n",
      "def main():\n",
      "    data_dir = 'data'\n",
      "    if not os.path.isdir(data_dir):\n",
      "        os.makedirs(data_dir)\n",
      "    x = np.linspace(0, 1, 100)\n",
      "    y = x\n",
      "    plt.figure(figsize=(6, 4))\n",
      "    plt.plot(x, y, label='y = x')\n",
      "    plt.xlabel('x')\n",
      "    plt.ylabel('y')\n",
      "    plt.title('Plot of y = x')\n",
      "    plt.grid(True)\n",
      "    plt.legend()\n",
      "    plt.tight_layout()\n",
      "    print('Plot of y = x from x=0 to 1')\n",
      "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
      "    filename = 'plot_y_equals_x_1_' + timestamp + '.png'\n",
      "    filepath = os.path.join(data_dir, filename)\n",
      "    plt.savefig(filepath, dpi=300)\n",
      "    plt.close()\n",
      "    print('Plot saved to ' + filepath)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: engineer_response_formatter\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `o3-mini-2025-01-31` does not exist.', 'type': 'NotFoundError', 'param': 'model', 'code': 404}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m tmp_path = Path.cwd()\n\u001b[32m     12\u001b[39m task = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mPlot the graph of y = x from x = 0 to 1\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m results = \u001b[43mone_shot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mengineer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengineer_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_engineer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mone_shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/cmbagent/workflows/one_shot.py:180\u001b[39m, in \u001b[36mone_shot\u001b[39m\u001b[34m(task, max_rounds, max_n_attempts, engineer_model, researcher_model, camb_context_model, default_llm_model, default_formatter_model, researcher_filename, agent, work_dir, api_keys, clear_work_dir, use_massgen, massgen_config, massgen_verbose, massgen_enable_logging, massgen_use_for_retries, custom_executor)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m researcher_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    178\u001b[39m     shared_context[\u001b[33m\"\u001b[39m\u001b[33mresearcher_filename\u001b[39m\u001b[33m\"\u001b[39m] = researcher_filename\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[43mcmbagent\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mone_shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_context\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m end_time = time.time()\n\u001b[32m    189\u001b[39m execution_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/cmbagent/cmbagent.py:711\u001b[39m, in \u001b[36mCMBAgent.solve\u001b[39m\u001b[34m(self, task, initial_agent, shared_context, mode, step, max_rounds)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Create the pattern\u001b[39;00m\n\u001b[32m    703\u001b[39m agent_pattern = AutoPattern(\n\u001b[32m    704\u001b[39m         agents=[agent.agent \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents],\n\u001b[32m    705\u001b[39m         initial_agent=\u001b[38;5;28mself\u001b[39m.get_agent_from_name(initial_agent),\n\u001b[32m   (...)\u001b[39m\u001b[32m    708\u001b[39m                               \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmain_cmbagent_chat\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    709\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m chat_result, context_variables, last_agent = \u001b[43minitiate_group_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthis_shared_context\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmain_task\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# user_agent=self.get_agent_from_name(\"admin\"),\u001b[39;49;00m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28mself\u001b[39m.final_context = copy.deepcopy(context_variables)\n\u001b[32m    720\u001b[39m \u001b[38;5;28mself\u001b[39m.last_agent = last_agent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/group/multi_agent_chat.py:110\u001b[39m, in \u001b[36minitiate_group_chat\u001b[39m\u001b[34m(pattern, messages, max_rounds, safeguard_policy, safeguard_llm_config, mask_llm_config)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m last_agent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo agent selected to start the conversation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m chat_result = \u001b[43mlast_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# print(\"!\"*70)\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# print(\"\\n in multi_agent_chat.py chat_result: \", chat_result)\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# print(\"!\"*70)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# initiate_chat only gathers cost from [sender, recipient],\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# but in group chat we need to include all participating agents\u001b[39;00m\n\u001b[32m    123\u001b[39m all_agents = \u001b[38;5;28mlist\u001b[39m(manager.groupchat.agents) + [manager]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1543\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         msg2send = \u001b[38;5;28mself\u001b[39m.generate_init_message(message, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[38;5;66;03m# print(\"XXXXXXXXXX==========  in conversable_agent.py chat messages: \", self.chat_messages)\u001b[39;00m\n\u001b[32m   1545\u001b[39m \u001b[38;5;66;03m# print(\"XXXXXXXXXX==========  in conversable_agent.py recipient name: \", recipient.name)\u001b[39;00m\n\u001b[32m   1546\u001b[39m \u001b[38;5;66;03m# print(\"XXXXXXXXXX==========  in conversable_agent.py sender name: \", _chat_info[\"sender\"])\u001b[39;00m\n\u001b[32m   1547\u001b[39m summary = \u001b[38;5;28mself\u001b[39m._summarize_chat(\n\u001b[32m   1548\u001b[39m     summary_method,\n\u001b[32m   1549\u001b[39m     summary_args,\n\u001b[32m   1550\u001b[39m     recipient,\n\u001b[32m   1551\u001b[39m     cache=cache,\n\u001b[32m   1552\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1181\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1179\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1184\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1185\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1296\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1298\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:3512\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3505\u001b[39m \u001b[38;5;66;03m# Check if this function should be triggered for this sender\u001b[39;00m\n\u001b[32m   3506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m   3507\u001b[39m     \u001b[38;5;66;03m# print(f\"Trigger matched for {reply_func.__name__}, executing...\")\u001b[39;00m\n\u001b[32m   3508\u001b[39m     \n\u001b[32m   3509\u001b[39m     \u001b[38;5;66;03m# Execute the reply function\u001b[39;00m\n\u001b[32m   3510\u001b[39m     \u001b[38;5;66;03m# print(\"\\n in conversable_agent.py generate_reply reply_func_tuple:\")\u001b[39;00m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;66;03m# print(json.dumps(reply_func_tuple, indent=4, default=str))\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3512\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3513\u001b[39m     \u001b[38;5;66;03m# print(f\"s final={final}, reply={reply}\")\u001b[39;00m\n\u001b[32m   3514\u001b[39m     \n\u001b[32m   3515\u001b[39m     \u001b[38;5;66;03m# Log the execution if logging is enabled\u001b[39;00m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3517\u001b[39m         \u001b[38;5;66;03m# print(\"Logging reply function execution\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/groupchat.py:1622\u001b[39m, in \u001b[36mGroupChatManager.run_chat\u001b[39m\u001b[34m(self, messages, sender, config)\u001b[39m\n\u001b[32m   1619\u001b[39m         reply = guardrails_reply\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1621\u001b[39m         \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m         reply = \u001b[43mspeaker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[32m   1625\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m groupchat.admin_name \u001b[38;5;129;01min\u001b[39;00m groupchat.agent_names:\n\u001b[32m   1626\u001b[39m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:3512\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3505\u001b[39m \u001b[38;5;66;03m# Check if this function should be triggered for this sender\u001b[39;00m\n\u001b[32m   3506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m   3507\u001b[39m     \u001b[38;5;66;03m# print(f\"Trigger matched for {reply_func.__name__}, executing...\")\u001b[39;00m\n\u001b[32m   3508\u001b[39m     \n\u001b[32m   3509\u001b[39m     \u001b[38;5;66;03m# Execute the reply function\u001b[39;00m\n\u001b[32m   3510\u001b[39m     \u001b[38;5;66;03m# print(\"\\n in conversable_agent.py generate_reply reply_func_tuple:\")\u001b[39;00m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;66;03m# print(json.dumps(reply_func_tuple, indent=4, default=str))\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3512\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3513\u001b[39m     \u001b[38;5;66;03m# print(f\"s final={final}, reply={reply}\")\u001b[39;00m\n\u001b[32m   3514\u001b[39m     \n\u001b[32m   3515\u001b[39m     \u001b[38;5;66;03m# Log the execution if logging is enabled\u001b[39;00m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3517\u001b[39m         \u001b[38;5;66;03m# print(\"Logging reply function execution\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:977\u001b[39m, in \u001b[36mConversableAgent.register_nested_chats.<locals>.wrapped_reply_func\u001b[39m\u001b[34m(recipient, messages, sender, config)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_reply_func\u001b[39m(recipient, messages=\u001b[38;5;28;01mNone\u001b[39;00m, sender=\u001b[38;5;28;01mNone\u001b[39;00m, config=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreply_func_from_nested_chats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:878\u001b[39m, in \u001b[36mConversableAgent._summary_from_nested_chats\u001b[39m\u001b[34m(chat_queue, recipient, messages, sender, config)\u001b[39m\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_to_run:\n\u001b[32m    877\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m res = \u001b[43minitiate_chats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_to_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;66;03m# We need to restore the chat queue message if it has been modified so that it will be the original message for subsequent uses\u001b[39;00m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m restore_chat_queue_message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/chat.py:213\u001b[39m, in \u001b[36minitiate_chats\u001b[39m\u001b[34m(chat_queue)\u001b[39m\n\u001b[32m    210\u001b[39m         __post_carryover_processing(chat_info)\n\u001b[32m    212\u001b[39m     sender = chat_info[\u001b[33m\"\u001b[39m\u001b[33msender\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     chat_res = \u001b[43msender\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mchat_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     finished_chats.append(chat_res)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m finished_chats\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1530\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1529\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# No breaks in the for loop, so we have reached max turns\u001b[39;00m\n\u001b[32m   1532\u001b[39m     iostream.send(\n\u001b[32m   1533\u001b[39m         TerminationEvent(\n\u001b[32m   1534\u001b[39m             termination_reason=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_turns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) reached\u001b[39m\u001b[33m\"\u001b[39m, sender=\u001b[38;5;28mself\u001b[39m, recipient=recipient\n\u001b[32m   1535\u001b[39m         )\n\u001b[32m   1536\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1181\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1179\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1184\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1185\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:1296\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1298\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:3512\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3505\u001b[39m \u001b[38;5;66;03m# Check if this function should be triggered for this sender\u001b[39;00m\n\u001b[32m   3506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m   3507\u001b[39m     \u001b[38;5;66;03m# print(f\"Trigger matched for {reply_func.__name__}, executing...\")\u001b[39;00m\n\u001b[32m   3508\u001b[39m     \n\u001b[32m   3509\u001b[39m     \u001b[38;5;66;03m# Execute the reply function\u001b[39;00m\n\u001b[32m   3510\u001b[39m     \u001b[38;5;66;03m# print(\"\\n in conversable_agent.py generate_reply reply_func_tuple:\")\u001b[39;00m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;66;03m# print(json.dumps(reply_func_tuple, indent=4, default=str))\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3512\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3513\u001b[39m     \u001b[38;5;66;03m# print(f\"s final={final}, reply={reply}\")\u001b[39;00m\n\u001b[32m   3514\u001b[39m     \n\u001b[32m   3515\u001b[39m     \u001b[38;5;66;03m# Log the execution if logging is enabled\u001b[39;00m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3517\u001b[39m         \u001b[38;5;66;03m# print(\"Logging reply function execution\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/groupchat.py:1622\u001b[39m, in \u001b[36mGroupChatManager.run_chat\u001b[39m\u001b[34m(self, messages, sender, config)\u001b[39m\n\u001b[32m   1619\u001b[39m         reply = guardrails_reply\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1621\u001b[39m         \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m         reply = \u001b[43mspeaker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[32m   1625\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m groupchat.admin_name \u001b[38;5;129;01min\u001b[39;00m groupchat.agent_names:\n\u001b[32m   1626\u001b[39m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:3512\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3505\u001b[39m \u001b[38;5;66;03m# Check if this function should be triggered for this sender\u001b[39;00m\n\u001b[32m   3506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m   3507\u001b[39m     \u001b[38;5;66;03m# print(f\"Trigger matched for {reply_func.__name__}, executing...\")\u001b[39;00m\n\u001b[32m   3508\u001b[39m     \n\u001b[32m   3509\u001b[39m     \u001b[38;5;66;03m# Execute the reply function\u001b[39;00m\n\u001b[32m   3510\u001b[39m     \u001b[38;5;66;03m# print(\"\\n in conversable_agent.py generate_reply reply_func_tuple:\")\u001b[39;00m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;66;03m# print(json.dumps(reply_func_tuple, indent=4, default=str))\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3512\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3513\u001b[39m     \u001b[38;5;66;03m# print(f\"s final={final}, reply={reply}\")\u001b[39;00m\n\u001b[32m   3514\u001b[39m     \n\u001b[32m   3515\u001b[39m     \u001b[38;5;66;03m# Log the execution if logging is enabled\u001b[39;00m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3517\u001b[39m         \u001b[38;5;66;03m# print(\"Logging reply function execution\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:2577\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config, **kwargs)\u001b[39m\n\u001b[32m   2574\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM call blocked by safeguard\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m-> \u001b[39m\u001b[32m2577\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2579\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2580\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2582\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;66;03m# Process LLM response\u001b[39;00m\n\u001b[32m   2585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/agentchat/conversable_agent.py:2670\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache, **kwargs)\u001b[39m\n\u001b[32m   2666\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2670\u001b[39m     response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2677\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2680\u001b[39m \u001b[38;5;66;03m# llm_client.print_usage_summary(mode=\"actual\")  # print actual usage summary, i.e., excluding cached usage\u001b[39;00m\n\u001b[32m   2681\u001b[39m \u001b[38;5;66;03m# Update dictionary containing all costs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/oai/client.py:1378\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1373\u001b[39m     \u001b[38;5;66;03m# import sys\u001b[39;00m\n\u001b[32m   1374\u001b[39m     \u001b[38;5;66;03m# sys.exit()\u001b[39;00m\n\u001b[32m   1377\u001b[39m request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;66;03m# cmbagent debug\u001b[39;00m\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cmbagent_debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/oai/client.py:742\u001b[39m, in \u001b[36mOpenAIClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    740\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m; pprint.pprint(params)\n\u001b[32m    741\u001b[39m     params.pop(\u001b[33m\"\u001b[39m\u001b[33mcheck_every_ms\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m     response = \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# import sys; sys.exit()\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/oai/client.py:458\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    457\u001b[39m     kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    460\u001b[39m     response_json = e.response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ag2/autogen/oai/client.py:547\u001b[39m, in \u001b[36mOpenAIClient.create.<locals>._create_or_parse\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    543\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = type_to_response_format_param(\n\u001b[32m    544\u001b[39m         \u001b[38;5;28mself\u001b[39m.response_format \u001b[38;5;129;01mor\u001b[39;00m params[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    545\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1294\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1285\u001b[39m     warnings.warn(\n\u001b[32m   1286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1288\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1289\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1290\u001b[39m     )\n\u001b[32m   1291\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1292\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1293\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/cmbagent/.venv/lib/python3.12/site-packages/openai/_base_client.py:1067\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1064\u001b[39m             err.response.read()\n\u001b[32m   1066\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `o3-mini-2025-01-31` does not exist.', 'type': 'NotFoundError', 'param': 'model', 'code': 404}}"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from cmbagent.workflows import one_shot\n",
    "\n",
    "local_engineer_config = local_llm_config[\"config_list\"][0]\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tmp_path = Path.cwd()\n",
    "\n",
    "task = \"\"\"\n",
    "Plot the graph of y = x from x = 0 to 1\n",
    "\"\"\"\n",
    "\n",
    "results = one_shot(\n",
    "    task,\n",
    "\n",
    "    max_rounds=5,\n",
    "    agent=\"engineer\",\n",
    "    engineer_model=local_engineer_config,\n",
    "    work_dir=str(tmp_path / \"one_shot\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Next step: add OSS for other engineering agents, e.g. `engineer_response_formatter`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
